{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "95EHJkaTKbrc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS5qgQaaO5Kz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4234559-bab9-4802-a647-93ee5273d3ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H879RcMUA0e-"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/train (1).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TbKbCuWQkmz",
        "outputId": "94596391-c68e-49ec-99ea-36e9cb8425d5"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       emotion                                             pixels\n",
            "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1            0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2            2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "...        ...                                                ...\n",
            "34882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...\n",
            "34883        3  178 174 172 173 181 188 191 194 196 199 200 20...\n",
            "34884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...\n",
            "34885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...\n",
            "34886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...\n",
            "\n",
            "[34887 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd7O13xrSbAQ"
      },
      "source": [
        "num_features = 64\n",
        "num_labels = 7\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "width, height = 48, 48\n",
        "\n",
        "datapoints = df['pixels'].tolist()\n",
        "\n",
        "#getting features for training\n",
        "X = []\n",
        "for xseq in datapoints:\n",
        "    xx = [int(xp) for xp in xseq.split(' ')]\n",
        "    xx = np.asarray(xx).reshape(width, height)\n",
        "    X.append(xx.astype('float32'))\n",
        "\n",
        "X = np.asarray(X)\n",
        "X = np.expand_dims(X, -1)\n",
        "\n",
        "#getting labels for training\n",
        "y = pd.get_dummies(df['emotion']).to_numpy()\n",
        "\n",
        "#storing them using numpy\n",
        "np.save('fdataX', X)\n",
        "np.save('flabels', y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl-17xXcRHT-"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=76)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6n1sTIsSPxf",
        "outputId": "fbfcc88d-0773-44ab-82c9-59a06a4203d2"
      },
      "source": [
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[ 26.]\n",
            "   [ 15.]\n",
            "   [ 15.]\n",
            "   ...\n",
            "   [ 59.]\n",
            "   [ 93.]\n",
            "   [124.]]\n",
            "\n",
            "  [[ 17.]\n",
            "   [ 15.]\n",
            "   [  9.]\n",
            "   ...\n",
            "   [ 14.]\n",
            "   [ 29.]\n",
            "   [107.]]\n",
            "\n",
            "  [[ 16.]\n",
            "   [ 19.]\n",
            "   [  4.]\n",
            "   ...\n",
            "   [ 32.]\n",
            "   [ 81.]\n",
            "   [ 77.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[184.]\n",
            "   [186.]\n",
            "   [188.]\n",
            "   ...\n",
            "   [ 57.]\n",
            "   [ 41.]\n",
            "   [ 84.]]\n",
            "\n",
            "  [[185.]\n",
            "   [187.]\n",
            "   [190.]\n",
            "   ...\n",
            "   [ 34.]\n",
            "   [ 60.]\n",
            "   [ 92.]]\n",
            "\n",
            "  [[189.]\n",
            "   [192.]\n",
            "   [194.]\n",
            "   ...\n",
            "   [ 37.]\n",
            "   [ 81.]\n",
            "   [ 93.]]]\n",
            "\n",
            "\n",
            " [[[246.]\n",
            "   [246.]\n",
            "   [242.]\n",
            "   ...\n",
            "   [164.]\n",
            "   [155.]\n",
            "   [157.]]\n",
            "\n",
            "  [[241.]\n",
            "   [240.]\n",
            "   [230.]\n",
            "   ...\n",
            "   [160.]\n",
            "   [193.]\n",
            "   [200.]]\n",
            "\n",
            "  [[240.]\n",
            "   [236.]\n",
            "   [221.]\n",
            "   ...\n",
            "   [229.]\n",
            "   [227.]\n",
            "   [226.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[252.]\n",
            "   [253.]\n",
            "   [253.]\n",
            "   ...\n",
            "   [216.]\n",
            "   [232.]\n",
            "   [234.]]\n",
            "\n",
            "  [[253.]\n",
            "   [253.]\n",
            "   [253.]\n",
            "   ...\n",
            "   [254.]\n",
            "   [252.]\n",
            "   [253.]]\n",
            "\n",
            "  [[254.]\n",
            "   [254.]\n",
            "   [254.]\n",
            "   ...\n",
            "   [253.]\n",
            "   [254.]\n",
            "   [253.]]]\n",
            "\n",
            "\n",
            " [[[255.]\n",
            "   [218.]\n",
            "   [111.]\n",
            "   ...\n",
            "   [ 67.]\n",
            "   [ 70.]\n",
            "   [ 93.]]\n",
            "\n",
            "  [[255.]\n",
            "   [192.]\n",
            "   [104.]\n",
            "   ...\n",
            "   [ 65.]\n",
            "   [ 75.]\n",
            "   [ 85.]]\n",
            "\n",
            "  [[254.]\n",
            "   [155.]\n",
            "   [ 98.]\n",
            "   ...\n",
            "   [ 65.]\n",
            "   [ 69.]\n",
            "   [ 80.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 27.]\n",
            "   [ 27.]\n",
            "   [ 18.]\n",
            "   ...\n",
            "   [153.]\n",
            "   [155.]\n",
            "   [157.]]\n",
            "\n",
            "  [[ 25.]\n",
            "   [ 27.]\n",
            "   [ 19.]\n",
            "   ...\n",
            "   [160.]\n",
            "   [154.]\n",
            "   [150.]]\n",
            "\n",
            "  [[ 24.]\n",
            "   [ 23.]\n",
            "   [ 18.]\n",
            "   ...\n",
            "   [130.]\n",
            "   [155.]\n",
            "   [151.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[242.]\n",
            "   [242.]\n",
            "   [242.]\n",
            "   ...\n",
            "   [242.]\n",
            "   [246.]\n",
            "   [255.]]\n",
            "\n",
            "  [[242.]\n",
            "   [240.]\n",
            "   [249.]\n",
            "   ...\n",
            "   [243.]\n",
            "   [253.]\n",
            "   [255.]]\n",
            "\n",
            "  [[242.]\n",
            "   [238.]\n",
            "   [255.]\n",
            "   ...\n",
            "   [254.]\n",
            "   [255.]\n",
            "   [255.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 19.]\n",
            "   [ 17.]\n",
            "   [ 16.]\n",
            "   ...\n",
            "   [129.]\n",
            "   [ 53.]\n",
            "   [104.]]\n",
            "\n",
            "  [[  8.]\n",
            "   [  8.]\n",
            "   [ 13.]\n",
            "   ...\n",
            "   [203.]\n",
            "   [ 17.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  4.]\n",
            "   [  4.]\n",
            "   [  3.]\n",
            "   ...\n",
            "   [192.]\n",
            "   [118.]\n",
            "   [  3.]]]\n",
            "\n",
            "\n",
            " [[[225.]\n",
            "   [223.]\n",
            "   [224.]\n",
            "   ...\n",
            "   [176.]\n",
            "   [189.]\n",
            "   [197.]]\n",
            "\n",
            "  [[211.]\n",
            "   [213.]\n",
            "   [217.]\n",
            "   ...\n",
            "   [152.]\n",
            "   [165.]\n",
            "   [177.]]\n",
            "\n",
            "  [[176.]\n",
            "   [154.]\n",
            "   [155.]\n",
            "   ...\n",
            "   [120.]\n",
            "   [122.]\n",
            "   [131.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  3.]\n",
            "   [  2.]\n",
            "   [  3.]\n",
            "   ...\n",
            "   [ 48.]\n",
            "   [  3.]\n",
            "   [  0.]]\n",
            "\n",
            "  [[  2.]\n",
            "   [  3.]\n",
            "   [  2.]\n",
            "   ...\n",
            "   [126.]\n",
            "   [ 87.]\n",
            "   [ 29.]]\n",
            "\n",
            "  [[  3.]\n",
            "   [  2.]\n",
            "   [  3.]\n",
            "   ...\n",
            "   [126.]\n",
            "   [137.]\n",
            "   [105.]]]\n",
            "\n",
            "\n",
            " [[[253.]\n",
            "   [247.]\n",
            "   [174.]\n",
            "   ...\n",
            "   [ 99.]\n",
            "   [222.]\n",
            "   [255.]]\n",
            "\n",
            "  [[253.]\n",
            "   [238.]\n",
            "   [161.]\n",
            "   ...\n",
            "   [ 73.]\n",
            "   [136.]\n",
            "   [238.]]\n",
            "\n",
            "  [[254.]\n",
            "   [219.]\n",
            "   [157.]\n",
            "   ...\n",
            "   [ 81.]\n",
            "   [ 84.]\n",
            "   [173.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[255.]\n",
            "   [255.]\n",
            "   [255.]\n",
            "   ...\n",
            "   [255.]\n",
            "   [255.]\n",
            "   [255.]]\n",
            "\n",
            "  [[255.]\n",
            "   [255.]\n",
            "   [255.]\n",
            "   ...\n",
            "   [255.]\n",
            "   [255.]\n",
            "   [255.]]\n",
            "\n",
            "  [[255.]\n",
            "   [255.]\n",
            "   [255.]\n",
            "   ...\n",
            "   [255.]\n",
            "   [255.]\n",
            "   [255.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udzpUTPwSUoB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mskWeu12St1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427ef62c-6b81-4ac9-e89c-f02a683fc7c6"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(2*2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 46, 46, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 23, 23, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 5, 5, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 5,905,863\n",
            "Trainable params: 5,902,151\n",
            "Non-trainable params: 3,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juAivozTPOl"
      },
      "source": [
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1hT8PAMVYiG",
        "outputId": "996473f0-f09e-4236-9754-f0c6084713ad"
      },
      "source": [
        "#training the model\n",
        "model.fit(np.array(X_train), np.array(y_train),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(np.array(X_test), np.array(y_test)),\n",
        "          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "437/437 [==============================] - 28s 45ms/step - loss: 2.3066 - accuracy: 0.1951 - precision_1: 0.1970 - recall_1: 0.0248 - val_loss: 1.8058 - val_accuracy: 0.2489 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.8418 - accuracy: 0.2460 - precision_1: 0.2473 - recall_1: 0.0014 - val_loss: 1.8227 - val_accuracy: 0.2528 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 3/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.8029 - accuracy: 0.2666 - precision_1: 0.4010 - recall_1: 0.0054 - val_loss: 1.8257 - val_accuracy: 0.2488 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
            "Epoch 4/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.7088 - accuracy: 0.3059 - precision_1: 0.6434 - recall_1: 0.0602 - val_loss: 1.6027 - val_accuracy: 0.3750 - val_precision_1: 0.8896 - val_recall_1: 0.0797\n",
            "Epoch 5/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.5939 - accuracy: 0.3645 - precision_1: 0.7400 - recall_1: 0.1442 - val_loss: 1.5863 - val_accuracy: 0.3663 - val_precision_1: 0.7162 - val_recall_1: 0.1725\n",
            "Epoch 6/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.5113 - accuracy: 0.4111 - precision_1: 0.7846 - recall_1: 0.1824 - val_loss: 1.4699 - val_accuracy: 0.4077 - val_precision_1: 0.8235 - val_recall_1: 0.1866\n",
            "Epoch 7/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.4501 - accuracy: 0.4340 - precision_1: 0.7965 - recall_1: 0.2009 - val_loss: 1.3671 - val_accuracy: 0.4600 - val_precision_1: 0.8604 - val_recall_1: 0.2111\n",
            "Epoch 8/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.4024 - accuracy: 0.4618 - precision_1: 0.7879 - recall_1: 0.2195 - val_loss: 1.3523 - val_accuracy: 0.4775 - val_precision_1: 0.8931 - val_recall_1: 0.1963\n",
            "Epoch 9/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.3825 - accuracy: 0.4691 - precision_1: 0.7783 - recall_1: 0.2345 - val_loss: 1.3157 - val_accuracy: 0.5135 - val_precision_1: 0.8903 - val_recall_1: 0.2082\n",
            "Epoch 10/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.3458 - accuracy: 0.4916 - precision_1: 0.7801 - recall_1: 0.2557 - val_loss: 1.3397 - val_accuracy: 0.4957 - val_precision_1: 0.9064 - val_recall_1: 0.1887\n",
            "Epoch 11/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.2893 - accuracy: 0.5169 - precision_1: 0.7855 - recall_1: 0.2818 - val_loss: 1.2358 - val_accuracy: 0.5380 - val_precision_1: 0.8321 - val_recall_1: 0.2797\n",
            "Epoch 12/100\n",
            "437/437 [==============================] - 19s 45ms/step - loss: 1.2770 - accuracy: 0.5275 - precision_1: 0.7812 - recall_1: 0.2915 - val_loss: 1.2061 - val_accuracy: 0.5335 - val_precision_1: 0.8325 - val_recall_1: 0.2906\n",
            "Epoch 13/100\n",
            "437/437 [==============================] - 19s 44ms/step - loss: 1.2526 - accuracy: 0.5305 - precision_1: 0.7773 - recall_1: 0.3079 - val_loss: 1.2974 - val_accuracy: 0.5203 - val_precision_1: 0.7875 - val_recall_1: 0.2485\n",
            "Epoch 14/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.2205 - accuracy: 0.5470 - precision_1: 0.7867 - recall_1: 0.3232 - val_loss: 1.1871 - val_accuracy: 0.5513 - val_precision_1: 0.7819 - val_recall_1: 0.3315\n",
            "Epoch 15/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.1904 - accuracy: 0.5602 - precision_1: 0.7740 - recall_1: 0.3444 - val_loss: 1.1482 - val_accuracy: 0.5754 - val_precision_1: 0.8419 - val_recall_1: 0.3121\n",
            "Epoch 16/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.1773 - accuracy: 0.5660 - precision_1: 0.7753 - recall_1: 0.3536 - val_loss: 1.1408 - val_accuracy: 0.5780 - val_precision_1: 0.8325 - val_recall_1: 0.3240\n",
            "Epoch 17/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.1530 - accuracy: 0.5768 - precision_1: 0.7795 - recall_1: 0.3676 - val_loss: 1.1246 - val_accuracy: 0.5817 - val_precision_1: 0.8173 - val_recall_1: 0.3449\n",
            "Epoch 18/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.1228 - accuracy: 0.5895 - precision_1: 0.7900 - recall_1: 0.3893 - val_loss: 1.2160 - val_accuracy: 0.5559 - val_precision_1: 0.7829 - val_recall_1: 0.3157\n",
            "Epoch 19/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.1170 - accuracy: 0.5883 - precision_1: 0.7778 - recall_1: 0.3855 - val_loss: 1.1408 - val_accuracy: 0.5847 - val_precision_1: 0.7771 - val_recall_1: 0.3951\n",
            "Epoch 20/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.0921 - accuracy: 0.6073 - precision_1: 0.7812 - recall_1: 0.4070 - val_loss: 1.1109 - val_accuracy: 0.5940 - val_precision_1: 0.7894 - val_recall_1: 0.3922\n",
            "Epoch 21/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.0833 - accuracy: 0.6068 - precision_1: 0.7831 - recall_1: 0.4126 - val_loss: 1.0927 - val_accuracy: 0.5980 - val_precision_1: 0.7834 - val_recall_1: 0.4189\n",
            "Epoch 22/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.0513 - accuracy: 0.6202 - precision_1: 0.7884 - recall_1: 0.4346 - val_loss: 1.1034 - val_accuracy: 0.6002 - val_precision_1: 0.7839 - val_recall_1: 0.4200\n",
            "Epoch 23/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 1.0179 - accuracy: 0.6329 - precision_1: 0.7921 - recall_1: 0.4558 - val_loss: 1.0758 - val_accuracy: 0.6028 - val_precision_1: 0.7796 - val_recall_1: 0.4339\n",
            "Epoch 24/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.9791 - accuracy: 0.6439 - precision_1: 0.7990 - recall_1: 0.4719 - val_loss: 1.1080 - val_accuracy: 0.5995 - val_precision_1: 0.7838 - val_recall_1: 0.4000\n",
            "Epoch 25/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.9955 - accuracy: 0.6410 - precision_1: 0.7983 - recall_1: 0.4695 - val_loss: 1.0701 - val_accuracy: 0.6135 - val_precision_1: 0.7819 - val_recall_1: 0.4450\n",
            "Epoch 26/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.9490 - accuracy: 0.6546 - precision_1: 0.8036 - recall_1: 0.5008 - val_loss: 1.0705 - val_accuracy: 0.6115 - val_precision_1: 0.7540 - val_recall_1: 0.4678\n",
            "Epoch 27/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.9318 - accuracy: 0.6634 - precision_1: 0.8028 - recall_1: 0.5179 - val_loss: 1.0627 - val_accuracy: 0.6179 - val_precision_1: 0.7631 - val_recall_1: 0.4732\n",
            "Epoch 28/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.9162 - accuracy: 0.6706 - precision_1: 0.8058 - recall_1: 0.5207 - val_loss: 1.0815 - val_accuracy: 0.6078 - val_precision_1: 0.7739 - val_recall_1: 0.4498\n",
            "Epoch 29/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.9187 - accuracy: 0.6698 - precision_1: 0.8067 - recall_1: 0.5189 - val_loss: 1.0517 - val_accuracy: 0.6185 - val_precision_1: 0.7522 - val_recall_1: 0.4872\n",
            "Epoch 30/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.8862 - accuracy: 0.6845 - precision_1: 0.8121 - recall_1: 0.5442 - val_loss: 1.0713 - val_accuracy: 0.6188 - val_precision_1: 0.7607 - val_recall_1: 0.4759\n",
            "Epoch 31/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.8723 - accuracy: 0.6843 - precision_1: 0.8132 - recall_1: 0.5499 - val_loss: 1.0318 - val_accuracy: 0.6251 - val_precision_1: 0.7596 - val_recall_1: 0.4871\n",
            "Epoch 32/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.8648 - accuracy: 0.6923 - precision_1: 0.8126 - recall_1: 0.5531 - val_loss: 1.1045 - val_accuracy: 0.6072 - val_precision_1: 0.7419 - val_recall_1: 0.4786\n",
            "Epoch 33/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.8513 - accuracy: 0.6957 - precision_1: 0.8148 - recall_1: 0.5658 - val_loss: 1.0883 - val_accuracy: 0.6221 - val_precision_1: 0.7631 - val_recall_1: 0.4898\n",
            "Epoch 34/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.8386 - accuracy: 0.6991 - precision_1: 0.8177 - recall_1: 0.5683 - val_loss: 1.0500 - val_accuracy: 0.6265 - val_precision_1: 0.7727 - val_recall_1: 0.4882\n",
            "Epoch 35/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.8090 - accuracy: 0.7121 - precision_1: 0.8273 - recall_1: 0.5933 - val_loss: 1.0875 - val_accuracy: 0.6311 - val_precision_1: 0.7407 - val_recall_1: 0.5235\n",
            "Epoch 36/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7926 - accuracy: 0.7198 - precision_1: 0.8262 - recall_1: 0.5997 - val_loss: 1.0543 - val_accuracy: 0.6350 - val_precision_1: 0.7641 - val_recall_1: 0.5004\n",
            "Epoch 37/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7812 - accuracy: 0.7195 - precision_1: 0.8312 - recall_1: 0.6062 - val_loss: 1.0918 - val_accuracy: 0.6280 - val_precision_1: 0.7475 - val_recall_1: 0.5057\n",
            "Epoch 38/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7699 - accuracy: 0.7249 - precision_1: 0.8295 - recall_1: 0.6101 - val_loss: 1.0821 - val_accuracy: 0.6278 - val_precision_1: 0.7346 - val_recall_1: 0.5228\n",
            "Epoch 39/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7589 - accuracy: 0.7366 - precision_1: 0.8383 - recall_1: 0.6245 - val_loss: 1.0513 - val_accuracy: 0.6361 - val_precision_1: 0.7467 - val_recall_1: 0.5289\n",
            "Epoch 40/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7357 - accuracy: 0.7390 - precision_1: 0.8320 - recall_1: 0.6381 - val_loss: 1.0747 - val_accuracy: 0.6376 - val_precision_1: 0.7535 - val_recall_1: 0.5239\n",
            "Epoch 41/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7306 - accuracy: 0.7402 - precision_1: 0.8409 - recall_1: 0.6399 - val_loss: 1.0685 - val_accuracy: 0.6321 - val_precision_1: 0.7693 - val_recall_1: 0.4894\n",
            "Epoch 42/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7335 - accuracy: 0.7354 - precision_1: 0.8328 - recall_1: 0.6357 - val_loss: 1.1144 - val_accuracy: 0.6306 - val_precision_1: 0.7251 - val_recall_1: 0.5391\n",
            "Epoch 43/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.6897 - accuracy: 0.7560 - precision_1: 0.8464 - recall_1: 0.6677 - val_loss: 1.1263 - val_accuracy: 0.6313 - val_precision_1: 0.7218 - val_recall_1: 0.5370\n",
            "Epoch 44/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7077 - accuracy: 0.7551 - precision_1: 0.8436 - recall_1: 0.6612 - val_loss: 1.0887 - val_accuracy: 0.6380 - val_precision_1: 0.7490 - val_recall_1: 0.5370\n",
            "Epoch 45/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.6888 - accuracy: 0.7583 - precision_1: 0.8422 - recall_1: 0.6648 - val_loss: 1.0871 - val_accuracy: 0.6413 - val_precision_1: 0.7584 - val_recall_1: 0.5344\n",
            "Epoch 46/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7208 - accuracy: 0.7475 - precision_1: 0.8454 - recall_1: 0.6467 - val_loss: 1.1712 - val_accuracy: 0.6144 - val_precision_1: 0.7092 - val_recall_1: 0.5183\n",
            "Epoch 47/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.7016 - accuracy: 0.7583 - precision_1: 0.8436 - recall_1: 0.6671 - val_loss: 1.1751 - val_accuracy: 0.6207 - val_precision_1: 0.7097 - val_recall_1: 0.5318\n",
            "Epoch 48/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.6282 - accuracy: 0.7803 - precision_1: 0.8567 - recall_1: 0.7023 - val_loss: 1.1020 - val_accuracy: 0.6453 - val_precision_1: 0.7315 - val_recall_1: 0.5615\n",
            "Epoch 49/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.6066 - accuracy: 0.7891 - precision_1: 0.8607 - recall_1: 0.7152 - val_loss: 1.1754 - val_accuracy: 0.6409 - val_precision_1: 0.7121 - val_recall_1: 0.5738\n",
            "Epoch 50/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.6089 - accuracy: 0.7905 - precision_1: 0.8614 - recall_1: 0.7195 - val_loss: 1.1754 - val_accuracy: 0.6330 - val_precision_1: 0.7127 - val_recall_1: 0.5582\n",
            "Epoch 51/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.6014 - accuracy: 0.7952 - precision_1: 0.8608 - recall_1: 0.7223 - val_loss: 1.1474 - val_accuracy: 0.6403 - val_precision_1: 0.7154 - val_recall_1: 0.5669\n",
            "Epoch 52/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5893 - accuracy: 0.7997 - precision_1: 0.8653 - recall_1: 0.7287 - val_loss: 1.1507 - val_accuracy: 0.6392 - val_precision_1: 0.7107 - val_recall_1: 0.5625\n",
            "Epoch 53/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5810 - accuracy: 0.8031 - precision_1: 0.8676 - recall_1: 0.7365 - val_loss: 1.1467 - val_accuracy: 0.6257 - val_precision_1: 0.7091 - val_recall_1: 0.5547\n",
            "Epoch 54/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5906 - accuracy: 0.7963 - precision_1: 0.8606 - recall_1: 0.7296 - val_loss: 1.1655 - val_accuracy: 0.6412 - val_precision_1: 0.7145 - val_recall_1: 0.5712\n",
            "Epoch 55/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5557 - accuracy: 0.8071 - precision_1: 0.8672 - recall_1: 0.7448 - val_loss: 1.1512 - val_accuracy: 0.6473 - val_precision_1: 0.7176 - val_recall_1: 0.5863\n",
            "Epoch 56/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5361 - accuracy: 0.8168 - precision_1: 0.8737 - recall_1: 0.7579 - val_loss: 1.1615 - val_accuracy: 0.6462 - val_precision_1: 0.7115 - val_recall_1: 0.5846\n",
            "Epoch 57/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5452 - accuracy: 0.8135 - precision_1: 0.8718 - recall_1: 0.7530 - val_loss: 1.1576 - val_accuracy: 0.6498 - val_precision_1: 0.7214 - val_recall_1: 0.5878\n",
            "Epoch 58/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5274 - accuracy: 0.8189 - precision_1: 0.8773 - recall_1: 0.7650 - val_loss: 1.2525 - val_accuracy: 0.6373 - val_precision_1: 0.7067 - val_recall_1: 0.5694\n",
            "Epoch 59/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5094 - accuracy: 0.8245 - precision_1: 0.8810 - recall_1: 0.7696 - val_loss: 1.2958 - val_accuracy: 0.6499 - val_precision_1: 0.7006 - val_recall_1: 0.5949\n",
            "Epoch 60/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5193 - accuracy: 0.8246 - precision_1: 0.8767 - recall_1: 0.7716 - val_loss: 1.1765 - val_accuracy: 0.6357 - val_precision_1: 0.6940 - val_recall_1: 0.5760\n",
            "Epoch 61/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5835 - accuracy: 0.8049 - precision_1: 0.8649 - recall_1: 0.7384 - val_loss: 1.1851 - val_accuracy: 0.6496 - val_precision_1: 0.7113 - val_recall_1: 0.5894\n",
            "Epoch 62/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.5118 - accuracy: 0.8252 - precision_1: 0.8786 - recall_1: 0.7715 - val_loss: 1.2704 - val_accuracy: 0.6317 - val_precision_1: 0.6827 - val_recall_1: 0.5884\n",
            "Epoch 63/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4787 - accuracy: 0.8384 - precision_1: 0.8843 - recall_1: 0.7916 - val_loss: 1.2289 - val_accuracy: 0.6482 - val_precision_1: 0.6984 - val_recall_1: 0.6000\n",
            "Epoch 64/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4792 - accuracy: 0.8384 - precision_1: 0.8871 - recall_1: 0.7918 - val_loss: 1.2573 - val_accuracy: 0.6397 - val_precision_1: 0.6895 - val_recall_1: 0.5966\n",
            "Epoch 65/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4665 - accuracy: 0.8388 - precision_1: 0.8850 - recall_1: 0.7934 - val_loss: 1.2277 - val_accuracy: 0.6459 - val_precision_1: 0.7053 - val_recall_1: 0.5987\n",
            "Epoch 66/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4699 - accuracy: 0.8418 - precision_1: 0.8858 - recall_1: 0.7960 - val_loss: 1.2654 - val_accuracy: 0.6482 - val_precision_1: 0.7078 - val_recall_1: 0.6010\n",
            "Epoch 67/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4633 - accuracy: 0.8501 - precision_1: 0.8925 - recall_1: 0.8035 - val_loss: 1.2318 - val_accuracy: 0.6492 - val_precision_1: 0.6977 - val_recall_1: 0.6029\n",
            "Epoch 68/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4518 - accuracy: 0.8479 - precision_1: 0.8869 - recall_1: 0.8049 - val_loss: 1.3176 - val_accuracy: 0.6336 - val_precision_1: 0.6771 - val_recall_1: 0.5929\n",
            "Epoch 69/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4404 - accuracy: 0.8508 - precision_1: 0.8925 - recall_1: 0.8117 - val_loss: 1.2305 - val_accuracy: 0.6495 - val_precision_1: 0.7019 - val_recall_1: 0.5964\n",
            "Epoch 70/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4462 - accuracy: 0.8471 - precision_1: 0.8904 - recall_1: 0.8078 - val_loss: 1.3510 - val_accuracy: 0.6480 - val_precision_1: 0.6920 - val_recall_1: 0.6096\n",
            "Epoch 71/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4786 - accuracy: 0.8417 - precision_1: 0.8828 - recall_1: 0.7962 - val_loss: 1.3003 - val_accuracy: 0.6298 - val_precision_1: 0.6767 - val_recall_1: 0.5904\n",
            "Epoch 72/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4535 - accuracy: 0.8475 - precision_1: 0.8900 - recall_1: 0.8045 - val_loss: 1.2599 - val_accuracy: 0.6420 - val_precision_1: 0.6977 - val_recall_1: 0.5990\n",
            "Epoch 73/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4247 - accuracy: 0.8576 - precision_1: 0.8930 - recall_1: 0.8194 - val_loss: 1.2753 - val_accuracy: 0.6455 - val_precision_1: 0.6944 - val_recall_1: 0.6079\n",
            "Epoch 74/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4246 - accuracy: 0.8599 - precision_1: 0.8959 - recall_1: 0.8239 - val_loss: 1.2623 - val_accuracy: 0.6447 - val_precision_1: 0.6935 - val_recall_1: 0.6103\n",
            "Epoch 75/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3895 - accuracy: 0.8727 - precision_1: 0.9033 - recall_1: 0.8380 - val_loss: 1.3420 - val_accuracy: 0.6467 - val_precision_1: 0.6876 - val_recall_1: 0.6162\n",
            "Epoch 76/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4076 - accuracy: 0.8625 - precision_1: 0.8954 - recall_1: 0.8297 - val_loss: 1.3146 - val_accuracy: 0.6424 - val_precision_1: 0.6854 - val_recall_1: 0.6040\n",
            "Epoch 77/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3981 - accuracy: 0.8672 - precision_1: 0.9012 - recall_1: 0.8338 - val_loss: 1.3205 - val_accuracy: 0.6397 - val_precision_1: 0.6844 - val_recall_1: 0.6019\n",
            "Epoch 78/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3936 - accuracy: 0.8721 - precision_1: 0.9019 - recall_1: 0.8389 - val_loss: 1.3323 - val_accuracy: 0.6472 - val_precision_1: 0.6874 - val_recall_1: 0.6139\n",
            "Epoch 79/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3785 - accuracy: 0.8727 - precision_1: 0.9032 - recall_1: 0.8439 - val_loss: 1.3111 - val_accuracy: 0.6465 - val_precision_1: 0.6869 - val_recall_1: 0.6125\n",
            "Epoch 80/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3630 - accuracy: 0.8789 - precision_1: 0.9080 - recall_1: 0.8507 - val_loss: 1.3871 - val_accuracy: 0.6449 - val_precision_1: 0.6820 - val_recall_1: 0.6159\n",
            "Epoch 81/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.4129 - accuracy: 0.8627 - precision_1: 0.8954 - recall_1: 0.8292 - val_loss: 1.3445 - val_accuracy: 0.6459 - val_precision_1: 0.6833 - val_recall_1: 0.6086\n",
            "Epoch 82/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3640 - accuracy: 0.8810 - precision_1: 0.9096 - recall_1: 0.8531 - val_loss: 1.3477 - val_accuracy: 0.6414 - val_precision_1: 0.6817 - val_recall_1: 0.6105\n",
            "Epoch 83/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3485 - accuracy: 0.8865 - precision_1: 0.9151 - recall_1: 0.8599 - val_loss: 1.3894 - val_accuracy: 0.6467 - val_precision_1: 0.6807 - val_recall_1: 0.6148\n",
            "Epoch 84/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3530 - accuracy: 0.8849 - precision_1: 0.9132 - recall_1: 0.8576 - val_loss: 1.3752 - val_accuracy: 0.6406 - val_precision_1: 0.6821 - val_recall_1: 0.6043\n",
            "Epoch 85/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3711 - accuracy: 0.8843 - precision_1: 0.9121 - recall_1: 0.8515 - val_loss: 1.4202 - val_accuracy: 0.6463 - val_precision_1: 0.6777 - val_recall_1: 0.6151\n",
            "Epoch 86/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3456 - accuracy: 0.8893 - precision_1: 0.9133 - recall_1: 0.8650 - val_loss: 1.3900 - val_accuracy: 0.6440 - val_precision_1: 0.6820 - val_recall_1: 0.6152\n",
            "Epoch 87/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3488 - accuracy: 0.8867 - precision_1: 0.9137 - recall_1: 0.8623 - val_loss: 1.4309 - val_accuracy: 0.6489 - val_precision_1: 0.6798 - val_recall_1: 0.6270\n",
            "Epoch 88/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3334 - accuracy: 0.8935 - precision_1: 0.9179 - recall_1: 0.8706 - val_loss: 1.4204 - val_accuracy: 0.6528 - val_precision_1: 0.6875 - val_recall_1: 0.6261\n",
            "Epoch 89/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3323 - accuracy: 0.8939 - precision_1: 0.9177 - recall_1: 0.8703 - val_loss: 1.3525 - val_accuracy: 0.6496 - val_precision_1: 0.6863 - val_recall_1: 0.6188\n",
            "Epoch 90/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3312 - accuracy: 0.8940 - precision_1: 0.9173 - recall_1: 0.8701 - val_loss: 1.4511 - val_accuracy: 0.6380 - val_precision_1: 0.6756 - val_recall_1: 0.6062\n",
            "Epoch 91/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3516 - accuracy: 0.8880 - precision_1: 0.9155 - recall_1: 0.8588 - val_loss: 1.3937 - val_accuracy: 0.6410 - val_precision_1: 0.6773 - val_recall_1: 0.6103\n",
            "Epoch 92/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3479 - accuracy: 0.8915 - precision_1: 0.9157 - recall_1: 0.8671 - val_loss: 1.4418 - val_accuracy: 0.6506 - val_precision_1: 0.6796 - val_recall_1: 0.6254\n",
            "Epoch 93/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3596 - accuracy: 0.8879 - precision_1: 0.9116 - recall_1: 0.8633 - val_loss: 1.3839 - val_accuracy: 0.6479 - val_precision_1: 0.6762 - val_recall_1: 0.6231\n",
            "Epoch 94/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3161 - accuracy: 0.8988 - precision_1: 0.9195 - recall_1: 0.8783 - val_loss: 1.3928 - val_accuracy: 0.6437 - val_precision_1: 0.6782 - val_recall_1: 0.6162\n",
            "Epoch 95/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3075 - accuracy: 0.9031 - precision_1: 0.9224 - recall_1: 0.8817 - val_loss: 1.5374 - val_accuracy: 0.6488 - val_precision_1: 0.6750 - val_recall_1: 0.6253\n",
            "Epoch 96/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3180 - accuracy: 0.8983 - precision_1: 0.9188 - recall_1: 0.8785 - val_loss: 1.4267 - val_accuracy: 0.6508 - val_precision_1: 0.6844 - val_recall_1: 0.6257\n",
            "Epoch 97/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3226 - accuracy: 0.8988 - precision_1: 0.9200 - recall_1: 0.8769 - val_loss: 1.4478 - val_accuracy: 0.6508 - val_precision_1: 0.6825 - val_recall_1: 0.6230\n",
            "Epoch 98/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3067 - accuracy: 0.9047 - precision_1: 0.9238 - recall_1: 0.8829 - val_loss: 1.4452 - val_accuracy: 0.6437 - val_precision_1: 0.6806 - val_recall_1: 0.6154\n",
            "Epoch 99/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.3042 - accuracy: 0.9039 - precision_1: 0.9227 - recall_1: 0.8844 - val_loss: 1.4574 - val_accuracy: 0.6377 - val_precision_1: 0.6704 - val_recall_1: 0.6138\n",
            "Epoch 100/100\n",
            "437/437 [==============================] - 20s 45ms/step - loss: 0.2999 - accuracy: 0.9064 - precision_1: 0.9256 - recall_1: 0.8868 - val_loss: 1.4713 - val_accuracy: 0.6457 - val_precision_1: 0.6783 - val_recall_1: 0.6237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe2b00f2f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg1p3edKV1sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7678ee6d-9f86-4ee6-c9e9-b5b5858c52c1"
      },
      "source": [
        "model.save('model.h5')\n",
        "print(\"Model saved as h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved as h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MhXnwT8Qn-F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}